
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>PKS Hands-on</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="claat-sample"
                  title="PKS Hands-on"
                  environment="web"
                  feedback-link="https://github.com/budougumi0617/claat-sample/issues">
    
      <google-codelab-step label="Prep" duration="0">
        <h2 is-upgraded>Version Information</h2>
<ul>
<li>Pivotal Cloud Foundry Operations Manager : 2.5.2 build.172</li>
<li>Pivotal Container Service (PKS) : 1.4.0 build.31</li>
<li>Terraform v0.11.13 + provider.google v2.6.0</li>
</ul>
<h2 is-upgraded>Prerequisites</h2>
<p>Your system needs the <code>gcloud</code> cli, as well as <code>terraform</code>:</p>
<pre>brew update
brew install Caskroom/cask/google-cloud-sdk
brew install terraform</pre>
<p>You also need the following CLIs to deploy PKS with BOSH Director.</p>
<ul>
<li><code>jq</code> CLI : https://stedolan.github.io/jq/</li>
</ul>
<pre># homebrew:
brew install jq</pre>
<ul>
<li><code>om</code> CLI : https://github.com/pivotal-cf/om/releases</li>
</ul>
<pre># homebrew:
brew tap starkandwayne/cf
brew install om</pre>
<ul>
<li><code>uaac</code> CLI : https://github.com/cloudfoundry/cf-uaac</li>
</ul>
<pre># Rubygems:
gem install cf-uaac</pre>
<h3 is-upgraded>Service Account</h3>
<p>You will need a key file for your service account to allow terraform to deploy resources. If you don&#39;t have one, you can create a service account and a key for it:</p>
<pre>export PROJECT_ID=&#34;XXXXXXXX&#34;
export ACCOUNT_NAME=&#34;YYYYYYYY&#34;
gcloud iam service-accounts create ${ACCOUNT_NAME} --display-name &#34;PKS Account&#34;
gcloud iam service-accounts keys create &#34;terraform.key.json&#34; --iam-account &#34;${ACCOUNT_NAME}@${PROJECT_ID}.iam.gserviceaccount.com&#34;
gcloud projects add-iam-policy-binding ${PROJECT_ID} --member &#34;serviceAccount:${ACCOUNT_NAME}@${PROJECT_ID}.iam.gserviceaccount.com&#34; --role &#39;roles/owner&#39;</pre>
<h3 is-upgraded>Var File</h3>
<p>Copy the stub content below into a terminal to create terraform.tfvars file. Make sure it&#39;s located in the root of this project. These vars will be used when you run terraform apply. You should fill in the stub values with the correct content.</p>
<pre>export PKS_ENV_PREFIX=&#34;XXXXXXXX&#34;
export OPS_IMAGE_URL=&#34;https://storage.googleapis.com/ops-manager-us/pcf-gcp-2.5.2-build.172.tar.gz&#34;</pre>
<pre>cat &lt;&lt; EOF &gt; terraform.tfvars
env_prefix = &#34;${PKS_ENV_PREFIX}&#34;
project = &#34;${PROJECT_ID}&#34;
region = &#34;asia-northeast1&#34;
zones = [&#34;asia-northeast1-a&#34;, &#34;asia-northeast1-b&#34;, &#34;asia-northeast1-c&#34;]
service_account_key = &lt;&lt;SERVICE_ACCOUNT_KEY
$(cat ./terraform.key.json)
SERVICE_ACCOUNT_KEY
nat_machine_type = &#34;n1-standard-1&#34;
opsman_image_url = &#34;${OPS_IMAGE_URL}&#34;
opsman_machine_type = &#34;n1-standard-2&#34;
EOF</pre>
<p>Make sure all variables are correctly set.</p>
<pre>cat terraform.tfvars</pre>
<h3 is-upgraded>Var Details</h3>
<ul>
<li>env_prefix: (required) An arbitrary unique name for namespacing resources. Max 23 characters.</li>
<li>project: (required) ID for your GCP project.</li>
<li>region: (required) Region in which to create resources (e.g. europe-west1)</li>
<li>zones: (required) Zones in which to create resources. Must be within the given region. Currently you must specify exactly 3 Zones for this terraform configuration to work. (e.g. [us-central1-a, us-central1-b, us-central1-c])</li>
<li>opsman_image_url (required) Source URL of the Ops Manager image you want to boot.</li>
<li>service_account_key: (required) Contents of your service account key file generated using the gcloud iam service-accounts keys create command.</li>
<li>nat_machine_type: (default: n1-standard-4) NAT machine type</li>
<li>opsman_machine_type: (default: n1-standard-2) Ops Manager machine type</li>
</ul>
<h2 is-upgraded>Running</h2>
<p>Note: please make sure you have created the <code>terraform.tfvars</code> file above as mentioned.</p>
<h3 is-upgraded>Standing up environment</h3>
<pre>terraform init
terraform plan -out=plan
terraform apply plan</pre>
<h3 is-upgraded>Tearing down environment</h3>
<pre>terraform destroy</pre>


      </google-codelab-step>
    
      <google-codelab-step label="Deploying BOSH Director" duration="0">
        <h2 is-upgraded>Install om CLI on Your PC</h2>
<p>For Mac:</p>
<pre>wget -q -O om https://github.com/pivotal-cf/om/releases/download/0.37.0/om-darwin
chmod +x om
mv om /usr/local/bin/</pre>
<p>For Linux:</p>
<pre>wget -q -O om https://github.com/pivotal-cf/om/releases/download/0.37.0/om-linux
chmod +x om
sudo mv om /usr/local/bin/</pre>
<h2 is-upgraded>Set Up Admin User for Ops Manager</h2>
<pre>OPS_MGR_USR=ops-admin
OPS_MGR_PWD=ops-password
OM_DECRYPTION_PWD=ops-password

OPSMAN_DOMAIN_OR_IP_ADDRESS=$(cat terraform.tfstate | jq -r &#39;.modules[0].resources.&#34;google_compute_address.ops-manager-public-ip&#34;.primary.attributes.address&#39;)
om --target https://$OPSMAN_DOMAIN_OR_IP_ADDRESS \
   --skip-ssl-validation \
   configure-authentication \
   --username $OPS_MGR_USR \
   --password $OPS_MGR_PWD \
   --decryption-passphrase $OM_DECRYPTION_PWD</pre>
<p>Output</p>
<pre>configuring internal userstore...
waiting for configuration to complete...
configuration complete</pre>
<p>Access <code>https://${OPSMAN_DOMAIN_OR_IP_ADDRESS}</code> from a web browser.</p>
<h2 is-upgraded>Configure Ops Manager</h2>
<p>Create a config file: <code>config-director.yml</code>.</p>
<pre>DIRECTOR_VM_TYPE=large.disk
INTERNET_CONNECTED=true
AUTH_JSON=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.AuthJSON.value&#39;)
OPSMAN_DOMAIN_OR_IP_ADDRESS=$(cat terraform.tfstate | jq -r &#39;.modules[0].resources.&#34;google_compute_address.ops-manager-public-ip&#34;.primary.attributes.address&#39;)
GCP_PROJECT_ID=$(echo $AUTH_JSON | tr -d &#39;[:cntrl:]&#39; | jq &#39;.project_id&#39;)
GCP_RESOURCE_PREFIX=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Default Deployment Tag&#34;&#39;| jq &#39;.value&#39;)
GCP_SERVICE_ACCOUNT_KEY=$(echo ${AUTH_JSON})
AVAILABILITY_ZONES=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Availability Zones&#34;.value | map({name: .})&#39; | tr -d &#39;\n&#39; | tr -d &#39;&#34;&#39;)
PKS_INFRASTRUCTURE_NETWORK_NAME=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Infrastructure Network Name&#34;.value&#39;)
PKS_INFRASTRUCTURE_IAAS_IDENTIFIER=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Infrastructure Network Google Network Name &#34;.value&#39;)
PKS_INFRASTRUCTURE_NETWORK_CIDR=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Infrastructure Network CIDR&#34;.value&#39;)
PKS_INFRASTRUCTURE_RESERVED_IP_RANGES=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Infrastructure Network Reserved IP Ranges&#34;.value&#39;)
PKS_INFRASTRUCTURE_DNS=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Infrastructure Network DNS&#34;.value&#39;)
PKS_INFRASTRUCTURE_GATEWAY=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Infrastructure Network Gateway&#34;.value&#39;)
PKS_INFRASTRUCTURE_AVAILABILITY_ZONES=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Availability Zones&#34;.value&#39; | tr -d &#39;\n&#39;)
PKS_MAIN_NETWORK_NAME=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Main Network Name&#34;.value&#39;)
PKS_MAIN_IAAS_IDENTIFIER=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Main Network Google Network Name &#34;.value&#39;)
PKS_MAIN_NETWORK_CIDR=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Main Network CIDR&#34;.value&#39;)
PKS_MAIN_RESERVED_IP_RANGES=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Main Network Reserved IP Ranges&#34;.value&#39;)
PKS_MAIN_DNS=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Main Network DNS&#34;.value&#39;)
PKS_MAIN_GATEWAY=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Main Network Gateway&#34;.value&#39;)
PKS_MAIN_AVAILABILITY_ZONES=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Availability Zones&#34;.value&#39; | tr -d &#39;\n&#39;)
PKS_SERVICES_NETWORK_NAME=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Service Network Name&#34;.value&#39;)
PKS_SERVICES_IAAS_IDENTIFIER=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Service Network Google Network Name &#34;.value&#39;)
PKS_SERVICES_NETWORK_CIDR=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Service Network CIDR&#34;.value&#39;)
PKS_SERVICES_RESERVED_IP_RANGES=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Services Network Reserved IP Ranges&#34;.value&#39;)
PKS_SERVICES_DNS=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Services Network DNS&#34;.value&#39;)
PKS_SERVICES_GATEWAY=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Services Network Gateway&#34;.value&#39;)
PKS_SERVICES_AVAILABILITY_ZONES=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Availability Zones&#34;.value&#39; | tr -d &#39;\n&#39;)
SINGLETON_AVAILABILITY_NETWORK=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Infrastructure Network Name&#34;.value&#39;)
SINGLETON_AVAILABILITY_ZONE=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Availability Zones&#34;.value | .[0]&#39;)

cat &lt;&lt;EOF &gt; config-director.yml
---
iaas-configuration:
  project: $GCP_PROJECT_ID
  default_deployment_tag: $GCP_RESOURCE_PREFIX
  auth_json: $(cat terraform.key.json | jq &#39;tostring&#39;)
director-configuration:
  ntp_servers_string: metadata.google.internal
  resurrector_enabled: true
  post_deploy_enabled: true
  database_type: internal
  blobstore_type: local
az-configuration: $AVAILABILITY_ZONES
networks-configuration:
  icmp_checks_enabled: false
  networks:
  - name: $PKS_INFRASTRUCTURE_NETWORK_NAME
    service_network: false
    subnets:
    - iaas_identifier: $PKS_INFRASTRUCTURE_IAAS_IDENTIFIER
      cidr: $PKS_INFRASTRUCTURE_NETWORK_CIDR
      reserved_ip_ranges: $PKS_INFRASTRUCTURE_RESERVED_IP_RANGES
      dns: $PKS_INFRASTRUCTURE_DNS
      gateway: $PKS_INFRASTRUCTURE_GATEWAY
      availability_zone_names: $PKS_INFRASTRUCTURE_AVAILABILITY_ZONES
  - name: $PKS_MAIN_NETWORK_NAME
    service_network: false
    subnets:
    - iaas_identifier: $PKS_MAIN_IAAS_IDENTIFIER
      cidr: $PKS_MAIN_NETWORK_CIDR
      reserved_ip_ranges: $PKS_MAIN_RESERVED_IP_RANGES
      dns: $PKS_MAIN_DNS
      gateway: $PKS_MAIN_GATEWAY
      availability_zone_names: $PKS_MAIN_AVAILABILITY_ZONES
  - name: $PKS_SERVICES_NETWORK_NAME
    service_network: true
    subnets:
    - iaas_identifier: $PKS_SERVICES_IAAS_IDENTIFIER
      cidr: $PKS_SERVICES_NETWORK_CIDR
      reserved_ip_ranges: $PKS_SERVICES_RESERVED_IP_RANGES
      dns: $PKS_SERVICES_DNS
      gateway: $PKS_SERVICES_GATEWAY
      availability_zone_names: $PKS_SERVICES_AVAILABILITY_ZONES
network-assignment:
  network:
    name: $SINGLETON_AVAILABILITY_NETWORK
  singleton_availability_zone:
    name: $SINGLETON_AVAILABILITY_ZONE
security-configuration:
  trusted_certificates: &#34;$OPS_MGR_TRUSTED_CERTS&#34;
  vm_password_type: generate
resource-configuration:
  director:
    instance_type:
      id: $DIRECTOR_VM_TYPE
    internet_connected: $INTERNET_CONNECTED
  compilation:
    instance_type:
      id: large.cpu
    internet_connected: $INTERNET_CONNECTED
EOF</pre>
<p>Configure the Ops Manager with <code>config-director.yml</code>.</p>
<pre>om --target https://$OPSMAN_DOMAIN_OR_IP_ADDRESS \
   --skip-ssl-validation \
   --username &#34;$OPS_MGR_USR&#34; \
   --password &#34;$OPS_MGR_PWD&#34; \
   configure-director \
   --config config-director.yml</pre>
<p>Output</p>
<pre>started configuring director options for bosh tile
finished configuring director options for bosh tile
started configuring availability zone options for bosh tile
finished configuring availability zone options for bosh tile
started configuring network options for bosh tile
finished configuring network options for bosh tile
started configuring network assignment options for bosh tile
finished configuring network assignment options for bosh tile
started configuring resource options for bosh tile
applying resource configuration for the following jobs:
  compilation
  director
finished configuring resource options for bosh tile</pre>
<h2 is-upgraded>Apply Changes</h2>
<pre>OPSMAN_DOMAIN_OR_IP_ADDRESS=$(cat terraform.tfstate | jq -r &#39;.modules[0].resources.&#34;google_compute_address.ops-manager-public-ip&#34;.primary.attributes.address&#39;)
om --target &#34;https://${OPSMAN_DOMAIN_OR_IP_ADDRESS}&#34; \
   --skip-ssl-validation \
   --username &#34;${OPS_MGR_USR}&#34; \
   --password &#34;${OPS_MGR_PWD}&#34; \
   apply-changes \
   --ignore-warnings</pre>


      </google-codelab-step>
    
      <google-codelab-step label="Deploying PKS" duration="0">
        <h2 is-upgraded>Upload PKS Tile to Ops Manager</h2>
<p>Access <code>https://network.pivotal.io/products/pivotal-container-service/</code>. Click <code>i</code> button next to the <code>pivotal-container-service-x.y.z-build.N.pivotal</code> to get the file name and download URL. Set them to the environment variables.</p>
<pre>FILENAME=pivotal-container-service-1.4.0-build.31.pivotal
DOWNLOAD_URL=https://network.pivotal.io/api/v2/products/pivotal-container-service/releases/354903/product_files/366115/download</pre>
<p>Access <code>https://network.pivotal.io/users/dashboard/edit-profile</code> and click <code>REQUEST NEW REFRESH TOKEN</code> to get a refresh token.</p>
<pre>REFRESH_TOKEN=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</pre>
<p>Get <code>ACCESS_TOKEN</code> with the command below.</p>
<pre>ACCESS_TOKEN=`curl -s https://network.pivotal.io/api/v2/authentication/access_tokens -d &#34;{\&#34;refresh_token\&#34;:\&#34;${REFRESH_TOKEN}\&#34;}&#34; | jq -r .access_token`</pre>
<p>Download <code>pivotal-container-service-x.y.z-build.N.pivotal</code> on the Ops Mamager.</p>
<pre>ZONE=`gcloud compute instances list --filter name:${PKS_ENV_PREFIX}-ops-manager | awk &#39;NR&gt;1 {print $2}&#39;`

gcloud compute ssh ubuntu@${PKS_ENV_PREFIX}-ops-manager \
    --zone ${ZONE} \
    --force-key-file-overwrite \
    --strict-host-key-checking=no \
    --quiet \
    --command &#34;wget -q -O &#34;${FILENAME}&#34; --header=&#39;Authorization: Bearer ${ACCESS_TOKEN}&#39; ${DOWNLOAD_URL}&#34;</pre>
<p>Install om command on the Ops Manager.</p>
<pre>gcloud compute ssh ubuntu@${PKS_ENV_PREFIX}-ops-manager \
    --zone ${ZONE} \
    --force-key-file-overwrite \
    --strict-host-key-checking=no \
    --quiet \
    --command &#34;wget -q -O om https://github.com/pivotal-cf/om/releases/download/0.37.0/om-linux &amp;&amp; chmod +x om &amp;&amp; sudo mv om /usr/local/bin/&#34;</pre>
<p>Upload pivotal-container-service-x.y.z-build.N.pivotal to the Ops Manager.</p>
<pre>PRODUCT_NAME=`basename $FILENAME .pivotal | python -c &#39;print(&#34;-&#34;.join(raw_input().split(&#34;-&#34;)[:-2]))&#39;` # pivotal-container-service
PRODUCT_VERSION=`basename $FILENAME .pivotal | python -c &#39;print(&#34;-&#34;.join(raw_input().split(&#34;-&#34;)[-2:]))&#39;` # 1.0.4-build.5

gcloud compute ssh ubuntu@${PKS_ENV_PREFIX}-ops-manager \
  --zone ${ZONE} \
  --force-key-file-overwrite \
  --strict-host-key-checking=no \
  --quiet \
  --command &#34;om --target https://localhost -k -u ${OPS_MGR_USR} -p ${OPS_MGR_PWD} --request-timeout 3600 upload-product -p ~/${FILENAME}&#34;</pre>
<p>Output</p>
<pre>processing product
beginning product upload to Ops Manager
 4.62 GiB / 4.62 GiB  100.00% 1m5s6s
2m28s elapsed, waiting for response from Ops Manager...
finished upload</pre>
<h2 is-upgraded>Staging PKS Tile</h2>
<pre>gcloud compute ssh ubuntu@${PKS_ENV_PREFIX}-ops-manager \
  --zone ${ZONE} \
  --force-key-file-overwrite \
  --strict-host-key-checking=no \
  --quiet \
  --command &#34;om --target https://localhost -k -u ${OPS_MGR_USR} -p ${OPS_MGR_PWD} stage-product -p ${PRODUCT_NAME} -v ${PRODUCT_VERSION}&#34;</pre>
<p>Output</p>
<pre>staging pivotal-container-service 1.4.0-build.31
finished staging</pre>
<h2 is-upgraded>Download Stemcell</h2>
<p>For ubuntu-xenial 250.25:</p>
<pre>SC_FILENAME=light-bosh-stemcell-250.25-google-kvm-ubuntu-xenial-go_agent.tgz
SC_DOWNLOAD_URL=https://network.pivotal.io/api/v2/products/stemcells-ubuntu-xenial/releases/331971/product_files/340983/download</pre>
<p>Download <code>light-bosh-stemcell-250.25-google-kvm-ubuntu-xenial-go_agent.tgz</code> on the Ops Manager.</p>
<pre>ACCESS_TOKEN=`curl -s https://network.pivotal.io/api/v2/authentication/access_tokens -d &#34;{\&#34;refresh_token\&#34;:\&#34;${REFRESH_TOKEN}\&#34;}&#34; | jq -r .access_token`

gcloud compute ssh ubuntu@${PKS_ENV_PREFIX}-ops-manager \
    --zone ${ZONE} \
    --force-key-file-overwrite \
    --strict-host-key-checking=no \
    --quiet \
    --command &#34;wget -q -O &#34;${SC_FILENAME}&#34; --header=&#39;Authorization: Bearer ${ACCESS_TOKEN}&#39; ${SC_DOWNLOAD_URL}&#34;</pre>
<p>Upload stemcell to the Ops Manager:</p>
<pre>gcloud compute ssh ubuntu@${PKS_ENV_PREFIX}-ops-manager \
  --zone ${ZONE} \
  --force-key-file-overwrite \
  --strict-host-key-checking=no \
  --quiet \
  --command &#34;om --target https://localhost -k -u ${OPS_MGR_USR} -p ${OPS_MGR_PWD} --request-timeout 3600 upload-stemcell -s ~/${SC_FILENAME}&#34;</pre>
<p>Output</p>
<pre>processing stemcell
beginning stemcell upload to Ops Manager
 20.30 KiB / 20.30 KiB  100.00% 0s
finished upload</pre>
<h2 is-upgraded>Configure PKS Tile</h2>
<p>Create a config file: <code>config-pks.yml</code>. If you don&#39;t want to assign public IP addresses to K8s cluster nodes, set a <code>- null</code> instead of <code>- public_ip</code> in the <code>.properties.vm_extensions:</code>.</p>
<pre>export ACCOUNT_NAME=YYYYYYYY # your terraform service account name ex) terraform-sa</pre>
<pre>om_generate_cert() (
  set -eu
  local domains=&#34;$1&#34;
  local data=$(echo $domains | jq --raw-input -c &#39;{&#34;domains&#34;: (. | split(&#34; &#34;))}&#39;)
  local response=$(
    om --target &#34;https://${OPSMAN_DOMAIN_OR_IP_ADDRESS}&#34; \
       --username &#34;$OPS_MGR_USR&#34; \
       --password &#34;$OPS_MGR_PWD&#34; \
       --skip-ssl-validation \
       curl \
       --silent \
       --path &#34;/api/v0/certificates/generate&#34; \
       -x POST \
       -d $data
    )
    echo &#34;$response&#34;
)

OPSMAN_DOMAIN_OR_IP_ADDRESS=$(cat terraform.tfstate | jq -r &#39;.modules[0].resources.&#34;google_compute_address.ops-manager-public-ip&#34;.primary.attributes.address&#39;)
PKS_API_IP=$(cat terraform.tfstate | jq -r &#39;.modules[0].resources.&#34;google_compute_address.pks-api-ip&#34;.primary.attributes.address&#39;)
PKS_DOMAIN=$(echo $PKS_API_IP | tr &#39;.&#39; &#39;-&#39;).sslip.io
AUTH_JSON=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.AuthJSON.value&#39;)
GCP_PROJECT_ID=$(echo $AUTH_JSON | tr -d &#39;[:cntrl:]&#39; | jq -r .project_id)
GCP_NETWORK=$(cat terraform.tfstate | jq -r &#39;.modules[0].resources.&#34;google_compute_network.pks-network&#34;.primary.id&#39;)
GCP_RESOURCE_PREFIX=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Default Deployment Tag&#34;.value&#39;)
PKS_MAIN_NETWORK_NAME=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Main Network Name&#34;.value&#39;)
PKS_SERVICES_NETWORK_NAME=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Service Network Name&#34;.value&#39;)
SINGLETON_AVAILABILITY_ZONE=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Availability Zones&#34;.value | .[0]&#39;)
AVAILABILITY_ZONES=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.&#34;Availability Zones&#34;.value | map({name: .})&#39; | tr -d &#39;\n&#39; | tr -d &#39;&#34;&#39;)
CERTIFICATES=$(om_generate_cert &#34;*.sslip.io *.x.sslip.io&#34;)
CERT_PEM=$(echo $CERTIFICATES  | tr &#39;\&#34;&#39; &#39;\n&#39; | awk &#39;/-----BEGIN CERTIFICATE-----/,/-----END CERTIFICATE-----/&#39; | sed &#39;s/^/        /&#39;)
KEY_PEM=$(echo $CERTIFICATES  | tr &#39;\&#34;&#39; &#39;\n&#39; | awk &#39;/-----BEGIN RSA PRIVATE KEY-----/,/-----END RSA PRIVATE KEY-----/&#39; | sed &#39;s/^/        /&#39;)
GCP_MASTER_SERVICE_ACCOUNT_KEY=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.pks_master_node_service_account_key.value&#39; | sed &#39;s/^/      /&#39;)
GCP_WORKER_SERVICE_ACCOUNT_KEY=$(cat terraform.tfstate | jq -r &#39;.modules[0].outputs.pks_worker_node_service_account_key.value&#39; | sed &#39;s/^/      /&#39;)
UAA_URL=api-${PKS_DOMAIN}
LB_NAME=&#34;tcp:${GCP_RESOURCE_PREFIX}-pks-api&#34;

cat &lt;&lt;EOF &gt; config-pks.yml
network-properties:
  singleton_availability_zone:
    name: asia-northeast1-a
  other_availability_zones:
  - name: asia-northeast1-a
  - name: asia-northeast1-b
  - name: asia-northeast1-c
  network:
    name: pks-main
  service_network:
    name: pks-services
product-properties:
  .pivotal-container-service.pks_tls:
    value:
      cert_pem: |
$CERT_PEM
      private_key_pem: |
$KEY_PEM
  .properties.pks_api_hostname:
    value: api-${PKS_DOMAIN}
  .properties.worker_max_in_flight: # The maximum number of non-canary instances to update in parallel within an availability zone.
    value: 1 # Maximum number of worker VMs created at a time (PKS API Service -&gt; Worker VM Max in Flight)
  .properties.plan1_selector:
    value: Plan Active
  .properties.plan1_selector.active.allow_privileged_containers:
    value: false
  .properties.plan1_selector.active.description:
    value: &#34;minimum resources for demo\r\nmaster node: 1\r\nworker node: 1&#34;
  .properties.plan1_selector.active.errand_vm_type:
    value: micro
  .properties.plan1_selector.active.master_az_placement:
    value:
    - asia-northeast1-a
  .properties.plan1_selector.active.master_instances:
    value: 1
  .properties.plan1_selector.active.master_persistent_disk_type:
    value: &#34;10240&#34;
  .properties.plan1_selector.active.master_vm_type:
    value: micro
  .properties.plan1_selector.active.max_worker_instances:
    value: 50
  .properties.plan1_selector.active.name:
    value: small
  .properties.plan1_selector.active.worker_az_placement:
    value:
    - asia-northeast1-a
    - asia-northeast1-b
    - asia-northeast1-c
  .properties.plan1_selector.active.worker_instances:
    value: 1
  .properties.plan1_selector.active.worker_persistent_disk_type:
    value: &#34;51200&#34;
  .properties.plan1_selector.active.worker_vm_type:
    value: medium
  .properties.plan2_selector:
    value: Plan Active
  .properties.plan2_selector.active.allow_privileged_containers:
    value: false
  .properties.plan2_selector.active.description:
    value: &#34;medium\r\nmaster node: 1\r\nworker node: 3&#34;
  .properties.plan2_selector.active.errand_vm_type:
    value: micro
  .properties.plan2_selector.active.eviction_hard:
    value: memory.available=100Mi, nodefs.available=10%, nodefs.inodesFree=5%
  .properties.plan2_selector.active.master_az_placement:
    value:
    - asia-northeast1-a
    - asia-northeast1-b
    - asia-northeast1-c
  .properties.plan2_selector.active.master_instances:
    value: 1
  .properties.plan2_selector.active.master_persistent_disk_type:
    value: &#34;10240&#34;
  .properties.plan2_selector.active.master_vm_type:
    value: small
  .properties.plan2_selector.active.max_worker_instances:
    value: 50
  .properties.plan2_selector.active.name:
    value: medium
  .properties.plan2_selector.active.system_reserved:
    value: memory=250Mi, cpu=150m
  .properties.plan2_selector.active.worker_az_placement:
    value:
    - asia-northeast1-a
    - asia-northeast1-b
    - asia-northeast1-c
  .properties.plan2_selector.active.worker_instances:
    value: 3
  .properties.plan2_selector.active.worker_persistent_disk_type:
    value: &#34;51200&#34;
  .properties.plan2_selector.active.worker_vm_type:
    value: medium
  .properties.plan3_selector:
    value: Plan Active
  .properties.plan3_selector.active.allow_privileged_containers:
    value: false
  .properties.plan3_selector.active.description:
    value: &#34;medium\r\nmaster node: 3\r\nworker node: 5&#34;
  .properties.plan3_selector.active.errand_vm_type:
    value: micro
  .properties.plan3_selector.active.eviction_hard:
    value: memory.available=100Mi, nodefs.available=10%, nodefs.inodesFree=5%
  .properties.plan3_selector.active.master_az_placement:
    value:
    - asia-northeast1-a
    - asia-northeast1-b
    - asia-northeast1-c
  .properties.plan3_selector.active.master_instances:
    value: 3
  .properties.plan3_selector.active.master_persistent_disk_type:
    value: &#34;10240&#34;
  .properties.plan3_selector.active.master_vm_type:
    value: small
  .properties.plan3_selector.active.max_worker_instances:
    value: 50
  .properties.plan3_selector.active.name:
    value: medium
  .properties.plan3_selector.active.system_reserved:
    value: memory=250Mi, cpu=150m
  .properties.plan3_selector.active.worker_az_placement:
    value:
    - asia-northeast1-a
    - asia-northeast1-b
    - asia-northeast1-c
  .properties.plan3_selector.active.worker_instances:
    value: 5
  .properties.plan3_selector.active.worker_persistent_disk_type:
    value: &#34;51200&#34;
  .properties.plan3_selector.active.worker_vm_type:
    value: medium
  .properties.plan4_selector:
    value: Plan Inactive
  .properties.plan4_selector.active.allow_privileged_containers:
    value: false
  .properties.plan4_selector.active.description:
    value: &#39;Example: This plan will configure a large kubernetes cluster for resource
      heavy workloads, or a high number of workloads.&#39;
  .properties.plan4_selector.active.master_instances:
    value: 3
  .properties.plan4_selector.active.max_worker_instances:
    value: 50
  .properties.plan4_selector.active.name:
    value: Plan-4
  .properties.plan4_selector.active.worker_instances:
    value: 5
  .properties.plan5_selector:
    value: Plan Inactive
  .properties.plan5_selector.active.allow_privileged_containers:
    value: false
  .properties.plan5_selector.active.description:
    value: &#39;Example: This plan will configure a large kubernetes cluster for resource
      heavy workloads, or a high number of workloads.&#39;
  .properties.plan5_selector.active.master_instances:
    value: 3
  .properties.plan5_selector.active.max_worker_instances:
    value: 50
  .properties.plan5_selector.active.name:
    value: Plan-5
  .properties.plan5_selector.active.worker_instances:
    value: 5
  .properties.plan6_selector:
    value: Plan Inactive
  .properties.plan6_selector.active.allow_privileged_containers:
    value: false
  .properties.plan6_selector.active.description:
    value: &#39;Example: This plan will configure a large kubernetes cluster for resource
      heavy workloads, or a high number of workloads.&#39;
  .properties.plan6_selector.active.master_instances:
    value: 3
  .properties.plan6_selector.active.max_worker_instances:
    value: 50
  .properties.plan6_selector.active.name:
    value: Plan-6
  .properties.plan6_selector.active.worker_instances:
    value: 5
  .properties.plan7_selector:
    value: Plan Inactive
  .properties.plan7_selector.active.allow_privileged_containers:
    value: false
  .properties.plan7_selector.active.description:
    value: &#39;Example: This plan will configure a large kubernetes cluster for resource
      heavy workloads, or a high number of workloads.&#39;
  .properties.plan7_selector.active.master_instances:
    value: 3
  .properties.plan7_selector.active.max_worker_instances:
    value: 50
  .properties.plan7_selector.active.name:
    value: Plan-7
  .properties.plan7_selector.active.worker_instances:
    value: 5
  .properties.plan8_selector:
    value: Plan Inactive
  .properties.plan8_selector.active.allow_privileged_containers:
    value: false
  .properties.plan8_selector.active.description:
    value: &#39;Example: This plan will configure a large kubernetes cluster for resource
      heavy workloads, or a high number of workloads.&#39;
  .properties.plan8_selector.active.master_instances:
    value: 3
  .properties.plan8_selector.active.max_worker_instances:
    value: 50
  .properties.plan8_selector.active.name:
    value: Plan-8
  .properties.plan8_selector.active.worker_instances:
    value: 5
  .properties.plan9_selector:
    value: Plan Inactive
  .properties.plan9_selector.active.allow_privileged_containers:
    value: false
  .properties.plan9_selector.active.description:
    value: &#39;Example: This plan will configure a large kubernetes cluster for resource
      heavy workloads, or a high number of workloads.&#39;
  .properties.plan9_selector.active.master_instances:
    value: 3
  .properties.plan9_selector.active.max_worker_instances:
    value: 50
  .properties.plan9_selector.active.name:
    value: Plan-9
  .properties.plan9_selector.active.worker_instances:
    value: 5
  .properties.plan10_selector:
    value: Plan Inactive
  .properties.plan10_selector.active.allow_privileged_containers:
    value: false
  .properties.plan10_selector.active.description:
    value: &#39;Example: This plan will configure a large kubernetes cluster for resource
      heavy workloads, or a high number of workloads.&#39;
  .properties.plan10_selector.active.master_instances:
    value: 3
  .properties.plan10_selector.active.max_worker_instances:
    value: 50
  .properties.plan10_selector.active.name:
    value: Plan-10
  .properties.plan10_selector.active.worker_instances:
    value: 5
  .properties.cloud_provider:
    value: GCP
  .properties.cloud_provider.gcp.project_id:
    value: $GCP_PROJECT_ID
  .properties.cloud_provider.gcp.network:
    value: $GCP_NETWORK
  .properties.cloud_provider.gcp.master_service_account:
    value: ${ACCOUNT_NAME}-pks-master-node@${GCP_PROJECT_ID}.iam.gserviceaccount.com
  .properties.cloud_provider.gcp.worker_service_account:
    value: ${PKS_ENV_PREFIX}-pks-worker-node@${GCP_PROJECT_ID}.iam.gserviceaccount.com
#  .properties.cloud_provider.vsphere.vcenter_master_creds:
#    value:
#      password: &#39;***&#39;
#  .properties.cloud_provider.azure.azure_cloud_name:
#    value: AzurePublicCloud
  .properties.syslog_selector:
    value: disabled
#  .properties.syslog_selector.enabled.transport_protocol:
#    value: tcp
#  .properties.syslog_selector.enabled.tls_enabled:
#    value: true
  .properties.pks-vrli:
    value: disabled
#  .properties.pks-vrli.enabled.skip_cert_verify:
#    value: false
#  .properties.pks-vrli.enabled.use_ssl:
#    value: true
  .properties.log_sink_resources_deploy:
    value: true
  .properties.metric_sink_resources_deploy:
    value: true
  .properties.network_selector:
    value: flannel
  .properties.network_selector.flannel.pod_network_cidr:
    value: 10.200.0.0/16
  .properties.network_selector.flannel.service_cluster_cidr:
    value: 10.100.200.0/24
#  .properties.network_selector.nsx.lb_size_large_supported:
#    value: true
#  .properties.network_selector.nsx.lb_size_medium_supported:
#    value: true
#  .properties.network_selector.nsx.nat_mode:
#    value: true
#  .properties.network_selector.nsx.network_automation:
#    value: true
#  .properties.network_selector.nsx.nsx-t-insecure:
#    value: false
#  .properties.network_selector.nsx.nsx-t-superuser-certificate:
#    value:
#      private_key_pem: &#39;***&#39;
#  .properties.proxy_selector.enabled.http_proxy_credentials:
#    value:
#      password: &#39;***&#39;
#  .properties.proxy_selector.enabled.https_proxy_credentials:
#    value:
#      password: &#39;***&#39;
  .properties.proxy_selector:
    value: Disabled
  .properties.vm_extensions:
    value:
    - public_ip # Networking -&gt; Enable outbound internet access (Warning: Not allowing internet access will require a NAT instance. To do so, set a null value instead of public_ip)
  .properties.uaa_pks_cli_access_token_lifetime:
    value: 600
  .properties.uaa_pks_cli_refresh_token_lifetime:
    value: 21600
  .properties.uaa_oidc:
    value: false
  .properties.uaa:
    value: internal
#  .properties.uaa.ldap.credentials:
#    value:
#      password: &#39;***&#39;
#  .properties.uaa.ldap.external_groups_whitelist:
#    value: &#39;*&#39;
#  .properties.uaa.ldap.group_search_filter:
#    value: member={0}
#  .properties.uaa.ldap.ldap_referrals:
#    value: follow
#  .properties.uaa.ldap.mail_attribute_name:
#    value: mail
#  .properties.uaa.ldap.search_filter:
#    value: cn={0}
  .properties.wavefront:
    value: disabled
#  .properties.wavefront.enabled.wavefront_token:
#    value:
#      secret: &#39;***&#39;
  .properties.pks-vrops:
    value: disabled # Configure PKS Monitoring Integration(s) -&gt; Deploy cAdvisor by VMware Integration
  .properties.telemetry_selector:
    value: disabled
#  .properties.telemetry_selector.enabled.billing_polling_interval:
#    value: 60
#  .properties.telemetry_selector.enabled.environment_provider:
#    value: none
#  .properties.telemetry_selector.enabled.telemetry_polling_interval:
#    value: 600
#  .properties.telemetry_selector.enabled.telemetry_url:
#    value: https://vcsa.vmware.com/ph
resource-config:
  pivotal-container-service:
    instances: automatic
    persistent_disk:
      size_mb: automatic
    instance_type:
      id: automatic
    internet_connected: $INTERNET_CONNECTED
    elb_names:
    - tcp:${PKS_ENV_PREFIX}-pks-api
EOF</pre>
<p>Configure the PKS Tile with <code>config-pks.yml</code>.</p>
<pre>om --target &#34;https://${OPSMAN_DOMAIN_OR_IP_ADDRESS}&#34; \
   --username &#34;$OPS_MGR_USR&#34; \
   --password &#34;$OPS_MGR_PWD&#34; \
   --skip-ssl-validation \
   configure-product \
   --product-name &#34;${PRODUCT_NAME}&#34; \
   --config config-pks.yml</pre>
<h2 is-upgraded>Apply Changes</h2>
<pre>OPSMAN_DOMAIN_OR_IP_ADDRESS=$(cat terraform.tfstate | jq -r &#39;.modules[0].resources.&#34;google_compute_address.ops-manager-public-ip&#34;.primary.attributes.address&#39;)
om --target &#34;https://${OPSMAN_DOMAIN_OR_IP_ADDRESS}&#34; \
   --skip-ssl-validation \
   --username &#34;${OPS_MGR_USR}&#34; \
   --password &#34;${OPS_MGR_PWD}&#34; \
   apply-changes \
   --ignore-warnings</pre>


      </google-codelab-step>
    
      <google-codelab-step label="Login to PKS API" duration="0">
        <h2 is-upgraded>Get API Endpoint</h2>
<pre>cat &lt;&lt; EOF
PKS API: https://api-${PKS_DOMAIN}:9021
UAA: https://api-${PKS_DOMAIN}:8443
EOF</pre>
<p>Output</p>
<pre>PKS API: https://api-xxx-xxx-xxx-xxx.sslip.io:9021
UAA: https://api-xxx-xxx-xxx-xxx.sslip.io:8443</pre>
<h2 is-upgraded>Get Admin Client Secret from the Ops Manager</h2>
<p>Get an <code>admin</code> client secret from the Ops Manager. Access the Ops Manager from a web browser -&gt; Click Pivotal Container Service Tile -&gt; Click <code>Credentials Tab</code> -&gt; Pks Uaa ManagementAdmin Client <code>Link to Credential</code>. Or you can get it with the command below.</p>
<pre>GUID=$(om \
    --target &#34;https://${OPSMAN_DOMAIN_OR_IP_ADDRESS}&#34; \
    --username &#34;$OPS_MGR_USR&#34; \
    --password &#34;$OPS_MGR_PWD&#34; \
    --skip-ssl-validation \
    curl \
    --silent \
    --path &#34;/api/v0/staged/products&#34; \
    -x GET \
    | jq -r &#39;.[] | select(.type == &#34;pivotal-container-service&#34;) | .guid&#39;
)
ADMIN_SECRET=$(om \
    --target &#34;https://${OPSMAN_DOMAIN_OR_IP_ADDRESS}&#34; \
    --username &#34;$OPS_MGR_USR&#34; \
    --password &#34;$OPS_MGR_PWD&#34; \
    --skip-ssl-validation \
    curl \
    --silent \
    --path &#34;/api/v0/deployed/products/${GUID}/credentials/.properties.pks_uaa_management_admin_client&#34; \
    -x GET \
    | jq -r &#39;.credential.value.secret&#39;
)</pre>
<h2 is-upgraded>Get Access Token for Admin Client</h2>
<pre>UAA_URL=https://api-${PKS_DOMAIN}:8443

gcloud compute ssh ubuntu@${PKS_ENV_PREFIX}-ops-manager \
  --zone ${ZONE} \
  --force-key-file-overwrite \
  --strict-host-key-checking=no \
  --quiet \
  --command &#34;uaac target ${UAA_URL} --skip-ssl-validation &amp;&amp; uaac token client get admin -s ${ADMIN_SECRET}&#34;</pre>
<p>Output</p>
<pre>Unknown key: Max-Age = 86400

Successfully fetched token via client credentials grant.
Target: https://api-xxx-xxx-xxx-xxx.sslip.io:8443
Context: admin, from client admin</pre>
<h2 is-upgraded>Create UAA User</h2>
<pre>PKS_USER=demo@example.com
PKS_PASSWORD=password1234

gcloud compute ssh ubuntu@${PKS_ENV_PREFIX}-ops-manager \
  --zone ${ZONE} \
  --force-key-file-overwrite \
  --strict-host-key-checking=no \
  --quiet \
  --command &#34;uaac user add ${PKS_USER} --emails ${PKS_USER} -p ${PKS_PASSWORD}&#34;</pre>
<p>Output</p>
<pre>user account successfully added</pre>
<h2 is-upgraded>Configure Scope</h2>
<p>Configure scopes to make uaa users accessible to pks commands. There are two types of scopes.</p>
<ul>
<li>pks.clusters.admin members have access to all the clusters.</li>
<li>pks.clusters.manage members have access only to the clusters that created themselves. Create pks.clusters.admin.</li>
</ul>
<pre>gcloud compute ssh ubuntu@${PKS_ENV_PREFIX}-ops-manager \
  --zone ${ZONE} \
  --force-key-file-overwrite \
  --strict-host-key-checking=no \
  --quiet \
  --command &#34;uaac member add pks.clusters.admin ${PKS_USER}&#34;</pre>
<p>Output</p>
<pre>success</pre>
<h2 is-upgraded>Login to PKS API</h2>
<pre>PKS_API_URL=https://api-${PKS_DOMAIN}:9021

pks login -k -a ${PKS_API_URL} -u ${PKS_USER} -p ${PKS_PASSWORD}</pre>
<p>Output</p>
<pre>API Endpoint: https://api-xxx-xxx-xxx-xxx.sslip.io:9021
User: demo@example.com</pre>


      </google-codelab-step>
    
      <google-codelab-step label="Create Cluster" duration="0">
        <p>Set a cluster name.</p>
<pre>CLUSTER_NAME=dev-cluster</pre>
<p>Before creating a K8s cluster, you need a load balancer with an external ip for K8s master nodes. Create a LB for K8s master nodes.</p>
<pre>GCP_REGION=$(cat terraform.tfstate | jq -r &#39;.modules[0].resources.&#34;google_compute_subnetwork.pks-subnet&#34;.primary.attributes.region&#39;)
gcloud compute addresses create ${CLUSTER_NAME}-master-api-ip --region ${GCP_REGION}
gcloud compute target-pools create ${CLUSTER_NAME}-master-api --region ${GCP_REGION}</pre>
<p>Get a LB External IP address.</p>
<pre>MASTER_EXTERNAL_IP=$(gcloud compute addresses describe ${CLUSTER_NAME}-master-api-ip --region ${GCP_REGION} --format json | jq -r .address)</pre>
<p>Create a K8s cluster.</p>
<pre>pks create-cluster ${CLUSTER_NAME} -e ${MASTER_EXTERNAL_IP} -p small</pre>
<pre>pks cluster ${CLUSTER_NAME}</pre>
<p>Attach K8s master nodes to the LB as backends.</p>
<pre>CLUSTER_UUID=$(pks cluster ${CLUSTER_NAME} --json | jq -r .uuid)
MASTER_INSTANCE_NAME=$(gcloud compute instances list --filter &#34;tags:service-instance-${CLUSTER_UUID}-master&#34; | awk &#39;NR&gt;1 {print $1}&#39;)
MASTER_INSTANCE_ZONE=$(gcloud compute instances list --filter &#34;tags:service-instance-${CLUSTER_UUID}-master&#34; | awk &#39;NR&gt;1 {print $2}&#39;)</pre>
<pre>gcloud compute target-pools add-instances ${CLUSTER_NAME}-master-api \
--instances ${MASTER_INSTANCE_NAME} \
--instances-zone ${MASTER_INSTANCE_ZONE} \
--region ${GCP_REGION}

gcloud compute forwarding-rules create ${CLUSTER_NAME}-master-api-8443 \
--region ${GCP_REGION} \
--address ${CLUSTER_NAME}-master-api-ip \
--target-pool ${CLUSTER_NAME}-master-api \
--ports 8443</pre>
<p>Fetch credentials for the K8s cluster.</p>
<pre>pks get-credentials ${CLUSTER_NAME}</pre>
<pre>kubectl cluster-info</pre>


      </google-codelab-step>
    
      <google-codelab-step label="Cheat Sheet" duration="0">
        <h2 is-upgraded>Login to PKS API</h2>
<pre>pks login -a API_URL -u USERNAME -p PASSWORD</pre>
<pre>pks plans</pre>
<p>Fetch credentials for the K8s cluster.</p>
<pre>pks get-credentials ${CLUSTER_NAME}</pre>
<h2 is-upgraded>om Command</h2>
<p>List config.</p>
<pre>ubuntu@env-prefix-ops-manager:~$ om --target https://localhost -k -u OPS_MGR_USR -p OPS_MGR_PWD available-products</pre>
<pre>ubuntu@env-prefix-ops-manager:~$ om --target https://localhost -k -u OPS_MGR_USR -p OPS_MGR_PWD staged-config --product-name pivotal-container-service</pre>
<p>Revert staged changes.</p>
<pre>ubuntu@env-prefix-ops-manager:~$ om --target https://localhost -k -u OPS_MGR_USR -p OPS_MGR_PWD revert-staged-changes --product-name pivotal-container-service</pre>
<p>Remove PKS and BOSH from the Ops Manager.</p>
<pre>ubuntu@env-prefix-ops-manager:~$ om --target https://localhost -k -u OPS_MGR_USR -p OPS_MGR_PWD delete-installation --product-name pivotal-container-service</pre>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>

</body>
</html>
